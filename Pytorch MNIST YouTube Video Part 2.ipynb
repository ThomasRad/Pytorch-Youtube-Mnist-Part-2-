{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a softmax classifer. First we need a package. \n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples : \n",
      " tensor([[0.0989, 0.1288, 0.1351, 0.1028, 0.0933, 0.0710, 0.1120, 0.0852, 0.0828,\n",
      "         0.0901],\n",
      "        [0.0893, 0.1134, 0.1106, 0.1242, 0.0845, 0.0920, 0.1319, 0.0753, 0.0812,\n",
      "         0.0976]])\n",
      "Sum : 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Now we use the softmax for our model. \n",
    "\n",
    "probs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "# Lets look at an example of it being applied. \n",
    "\n",
    "print('Some examples : \\n', probs[:2].data)\n",
    "\n",
    "# Add the prob for a output row. \n",
    "\n",
    "print(\"Sum :\", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 6, 6, 6, 6, 2, 6, 2, 6, 9, 2, 6, 6, 2, 1, 5, 6, 1, 6, 2, 6, 6, 2, 2,\n",
      "        2, 1, 1, 2, 6, 1, 1, 6, 2, 6, 2, 1, 2, 2, 8, 2, 2, 6, 2, 3, 2, 2, 6, 6,\n",
      "        6, 2, 6, 6, 6, 1, 6, 6, 2, 6, 1, 6, 6, 2, 3, 8, 6, 3, 0, 1, 6, 1, 2, 2,\n",
      "        2, 1, 6, 2, 6, 2, 6, 6, 2, 1, 5, 6, 2, 8, 6, 6, 0, 6, 2, 2, 2, 3, 6, 2,\n",
      "        9, 6, 2, 2, 2, 1, 6, 1, 6, 9, 3, 6, 2, 6, 1, 1, 3, 3, 6, 6, 1, 6, 1, 1,\n",
      "        2, 2, 6, 6, 2, 6, 6, 6])\n",
      "tensor([0.1351, 0.1319, 0.1234, 0.1483, 0.1518, 0.1363, 0.1259, 0.1329, 0.1347,\n",
      "        0.1166, 0.1485, 0.1650, 0.1348, 0.1558, 0.1265, 0.1228, 0.1243, 0.1297,\n",
      "        0.1326, 0.1257, 0.1261, 0.1513, 0.1407, 0.1432, 0.1532, 0.1351, 0.1286,\n",
      "        0.1561, 0.1321, 0.1239, 0.1420, 0.1373, 0.1227, 0.1277, 0.1537, 0.1408,\n",
      "        0.1303, 0.1307, 0.1484, 0.1445, 0.1384, 0.1385, 0.1503, 0.1284, 0.1797,\n",
      "        0.1285, 0.1171, 0.1281, 0.1593, 0.1505, 0.1303, 0.1246, 0.1391, 0.1338,\n",
      "        0.1468, 0.1371, 0.1507, 0.1223, 0.1472, 0.1554, 0.1264, 0.1325, 0.1265,\n",
      "        0.1225, 0.1214, 0.1360, 0.1310, 0.1306, 0.1361, 0.1223, 0.1339, 0.1248,\n",
      "        0.1145, 0.1446, 0.1136, 0.1456, 0.1238, 0.1421, 0.1233, 0.1445, 0.1527,\n",
      "        0.1318, 0.1264, 0.1515, 0.1184, 0.1249, 0.1252, 0.1604, 0.1230, 0.1381,\n",
      "        0.1420, 0.1530, 0.1227, 0.1130, 0.1236, 0.1311, 0.1318, 0.1507, 0.1486,\n",
      "        0.1703, 0.1392, 0.1276, 0.1448, 0.1434, 0.1367, 0.1318, 0.1283, 0.1158,\n",
      "        0.1155, 0.1361, 0.1414, 0.1409, 0.1213, 0.1399, 0.1194, 0.1296, 0.1445,\n",
      "        0.1517, 0.1728, 0.1312, 0.1362, 0.1195, 0.1305, 0.1347, 0.1270, 0.1790,\n",
      "        0.1412, 0.1728], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we need to take the largest probablity and associate it with a label from 0 to 9 \n",
    "\n",
    "max_probs, preds = torch.max(probs, dim = 1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an accuracy for our model and the pred. \n",
    "\n",
    "def acc(outputs,labels):\n",
    "    probability, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item()/ len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model with random Weights and Bias has an accuracy of 9%. \n",
    "\n",
    "acc(outputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use crossentropy as our loss function to outdate our model. \n",
    "\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3112, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Apply it to the data we have. \n",
    "\n",
    "loss = loss_fn(outputs,labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now create the model and all the moving parts to it. \n",
    "\n",
    "def fit(epochs, lr, model, train_loader,val_loader, opt_func = torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    history = [] # recording the results \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #Training\n",
    "        for batch in train_loader: \n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "        #Valid\n",
    "        \n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "        \n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the evaluate function. \n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create our final Mnistmodel that we will test. \n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Linear = nn.Linear(input_size,num_classes)\n",
    "        \n",
    "    def forward(self,xb):\n",
    "        xb = xb.reshape(-1,784)\n",
    "        out = self.Linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch):\n",
    "        images, labels = batch\n",
    "        out = self(batch)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        accu = acc(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': accu}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_losses = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_losses.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self,epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch,result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.284787893295288, 'val_acc': 0.1461629718542099}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the function on a sample set \n",
    "\n",
    "results1 = evaluate(model,val_loader)\n",
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
